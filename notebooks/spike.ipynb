{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Video Presentation Transcriber\n",
    "\n",
    "Extracts visually distinct frames from a video and uses a local LLM\n",
    "(via Ollama) to transcribe/describe the content. Also extracts and\n",
    "transcribes audio using Whisper."
   ],
   "id": "2481e92fcfcdea7b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T11:06:48.619805Z",
     "start_time": "2025-12-01T11:06:48.616183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# \"\"\"\n",
    "# Video Presentation Transcriber\n",
    "#\n",
    "# Extracts visually distinct frames from a video and uses a local LLM\n",
    "# (via Ollama) to transcribe/describe the content. Also extracts and\n",
    "# transcribes audio using Whisper.\n",
    "#\n",
    "# Requirements:\n",
    "#     pip install opencv-python numpy requests pillow faster-whisper\n",
    "#\n",
    "# Usage:\n",
    "#     from video_transcriber import VideoTranscriber\n",
    "#\n",
    "#     transcriber = VideoTranscriber()\n",
    "#\n",
    "#     # Define regions to ignore (e.g., presenter picture-in-picture windows)\n",
    "#     # Format: (x, y, width, height) as fractions of frame size (0-1)\n",
    "#     ignore_regions = [\n",
    "#         (0.0, 0.7, 0.2, 0.3),   # Bottom-left corner\n",
    "#         (0.8, 0.7, 0.2, 0.3),   # Bottom-right corner\n",
    "#     ]\n",
    "#\n",
    "#     results = transcriber.process_video(\n",
    "#         \"presentation.mp4\",\n",
    "#         ignore_regions=ignore_regions,\n",
    "#     )\n",
    "# \"\"\""
   ],
   "id": "3fbdfd7dde2a9f06",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T11:06:49.519380Z",
     "start_time": "2025-12-01T11:06:48.665827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import requests\n",
    "import base64\n",
    "import json\n",
    "import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Iterator, Optional\n",
    "import os"
   ],
   "id": "ef04334087012ba0",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T11:06:49.666079Z",
     "start_time": "2025-12-01T11:06:49.579847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class AudioSegment:\n",
    "    \"\"\"A segment of transcribed audio.\"\"\"\n",
    "    start_seconds: float\n",
    "    end_seconds: float\n",
    "    text: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FrameResult:\n",
    "    \"\"\"Holds a frame and its transcription.\"\"\"\n",
    "    frame_number: int\n",
    "    timestamp_seconds: float\n",
    "    image: np.ndarray\n",
    "    transcription: Optional[str] = None\n",
    "    audio_segments: list[AudioSegment] = field(default_factory=list)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TranscriptResult:\n",
    "    \"\"\"Complete transcript with visual and audio components.\"\"\"\n",
    "    frames: list[FrameResult]\n",
    "    audio_segments: list[AudioSegment]"
   ],
   "id": "658ddfb53565259b",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T11:06:49.765066Z",
     "start_time": "2025-12-01T11:06:49.681745Z"
    }
   },
   "cell_type": "code",
   "source": "class VideoTranscriber:\n    def __init__(\n        self,\n        ollama_url: str = \"http://localhost:11434\",\n        vision_model: str = \"llava\",\n        whisper_model: str = \"base\",\n        similarity_threshold: float = 0.92,\n        min_frame_interval: int = 15,\n        image_quality: int = 100,  # NEW: configurable image quality\n        use_png: bool = False,  # NEW: option to use PNG (lossless)\n        timeout: int = 300,  # NEW: timeout in seconds for vision model API calls\n    ):\n        \"\"\"\n        Initialize the video transcriber.\n\n        Args:\n            ollama_url: Base URL for Ollama API\n            vision_model: Vision model to use (llava, bakllava, llava-llama3, etc.)\n            whisper_model: Whisper model size (tiny, base, small, medium, large-v3)\n            similarity_threshold: Frames more similar than this are considered duplicates (0-1)\n            min_frame_interval: Minimum frames between captures (avoids transition frames)\n            image_quality: JPEG quality (1-100, higher is better, default 100)\n            use_png: Use PNG encoding instead of JPEG (lossless but larger)\n            timeout: Timeout in seconds for vision model API calls (default 300)\n        \"\"\"\n        self.ollama_url = ollama_url.rstrip(\"/\")\n        self.vision_model = vision_model\n        self.whisper_model = whisper_model\n        self.similarity_threshold = similarity_threshold\n        self.min_frame_interval = min_frame_interval\n        self.image_quality = image_quality\n        self.use_png = use_png\n        self.timeout = timeout\n        self._whisper = None\n\n    def _get_whisper(self):\n        \"\"\"Lazy-load Whisper model.\"\"\"\n        if self._whisper is None:\n            try:\n                from faster_whisper import WhisperModel\n                print(f\"Loading Whisper model: {self.whisper_model}\")\n                # Use CPU by default; change to \"cuda\" if you have GPU\n                self._whisper = WhisperModel(\n                    self.whisper_model,\n                    device=\"auto\",\n                    compute_type=\"auto\",\n                )\n            except ImportError:\n                raise ImportError(\n                    \"faster-whisper not installed. Run: pip install faster-whisper\"\n                )\n        return self._whisper\n\n    def _apply_ignore_mask(\n        self,\n        frame: np.ndarray,\n        ignore_regions: list[tuple[float, float, float, float]],\n    ) -> np.ndarray:\n        \"\"\"\n        Apply mask to ignore specified regions (e.g., presenter windows).\n\n        Args:\n            frame: Input frame\n            ignore_regions: List of (x, y, width, height) as fractions (0-1)\n\n        Returns:\n            Frame with ignored regions blacked out\n        \"\"\"\n        if not ignore_regions:\n            return frame\n\n        masked = frame.copy()\n        h, w = frame.shape[:2]\n\n        for (rx, ry, rw, rh) in ignore_regions:\n            x1 = int(rx * w)\n            y1 = int(ry * h)\n            x2 = int((rx + rw) * w)\n            y2 = int((ry + rh) * h)\n\n            # Black out the region\n            masked[y1:y2, x1:x2] = 0\n\n        return masked\n\n    def _compute_frame_hash(self, frame: np.ndarray, hash_size: int = 16) -> np.ndarray:\n        \"\"\"\n        Compute a perceptual hash for change detection.\n        Uses average hash - fast and effective for slide detection.\n        \"\"\"\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        resized = cv2.resize(gray, (hash_size, hash_size), interpolation=cv2.INTER_AREA)\n        mean_val = resized.mean()\n        return (resized > mean_val).flatten()\n\n    def _frames_similar(self, hash1: np.ndarray, hash2: np.ndarray) -> float:\n        \"\"\"Compute similarity between two frame hashes (0-1).\"\"\"\n        return np.mean(hash1 == hash2)\n\n    def preview_ignore_regions(\n        self,\n        video_path: str,\n        ignore_regions: list[tuple[float, float, float, float]],\n        output_path: str = \"preview_mask.jpg\",\n        frame_number: int = 100,\n    ):\n        \"\"\"\n        Save a preview image showing which regions will be ignored.\n        Useful for tuning the ignore_regions parameter.\n\n        Args:\n            video_path: Path to video\n            ignore_regions: Regions to ignore\n            output_path: Where to save preview image\n            frame_number: Which frame to use for preview\n        \"\"\"\n        cap = cv2.VideoCapture(video_path)\n        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n        ret, frame = cap.read()\n        cap.release()\n\n        if not ret:\n            raise ValueError(f\"Could not read frame {frame_number}\")\n\n        # Draw rectangles showing ignored regions\n        h, w = frame.shape[:2]\n        overlay = frame.copy()\n\n        for (rx, ry, rw, rh) in ignore_regions:\n            x1 = int(rx * w)\n            y1 = int(ry * h)\n            x2 = int((rx + rw) * w)\n            y2 = int((ry + rh) * h)\n\n            # Red semi-transparent overlay\n            cv2.rectangle(overlay, (x1, y1), (x2, y2), (0, 0, 255), -1)\n\n        # Blend overlay\n        preview = cv2.addWeighted(overlay, 0.4, frame, 0.6, 0)\n\n        # Add text labels\n        cv2.putText(\n            preview, \"Red regions will be IGNORED for change detection\",\n            (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2\n        )\n\n        cv2.imwrite(output_path, preview)\n        print(f\"Preview saved to: {output_path}\")\n\n        return preview\n\n    def extract_distinct_frames(\n        self,\n        video_path: str,\n        sample_interval: int = 30,\n        ignore_regions: Optional[list[tuple[float, float, float, float]]] = None,\n    ) -> Iterator[FrameResult]:\n        \"\"\"\n        Extract visually distinct frames from video.\n\n        Args:\n            video_path: Path to video file\n            sample_interval: Check every N frames for changes\n            ignore_regions: Regions to mask out before change detection\n\n        Yields:\n            FrameResult objects for each distinct frame\n        \"\"\"\n        cap = cv2.VideoCapture(video_path)\n        if not cap.isOpened():\n            raise ValueError(f\"Could not open video: {video_path}\")\n\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        duration = total_frames / fps\n\n        print(f\"Video: {total_frames} frames at {fps:.1f} FPS ({duration:.1f}s / {duration/60:.1f}min)\")\n        if ignore_regions:\n            print(f\"Ignoring {len(ignore_regions)} region(s) for change detection\")\n\n        last_hash = None\n        last_captured_frame = -self.min_frame_interval\n        frame_count = 0\n\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            frame_count += 1\n\n            # Only check at sample intervals\n            if frame_count % sample_interval != 0:\n                continue\n\n            # Apply mask before computing hash (but keep original frame for output)\n            masked_frame = self._apply_ignore_mask(frame, ignore_regions or [])\n            current_hash = self._compute_frame_hash(masked_frame)\n\n            # Check if frame is sufficiently different\n            is_distinct = False\n            if last_hash is None:\n                is_distinct = True\n            else:\n                similarity = self._frames_similar(current_hash, last_hash)\n                if similarity < self.similarity_threshold:\n                    if (frame_count - last_captured_frame) >= self.min_frame_interval:\n                        is_distinct = True\n\n            if is_distinct:\n                timestamp = frame_count / fps\n                print(f\"  Captured frame {frame_count} at {timestamp:.1f}s ({timestamp/60:.1f}min)\")\n\n                yield FrameResult(\n                    frame_number=frame_count,\n                    timestamp_seconds=timestamp,\n                    image=frame.copy(),  # Original frame, not masked\n                )\n\n                last_hash = current_hash\n                last_captured_frame = frame_count\n\n        cap.release()\n\n    def extract_audio(self, video_path: str, output_path: Optional[str] = None) -> str:\n        \"\"\"\n        Extract audio from video using ffmpeg.\n\n        Args:\n            video_path: Path to video file\n            output_path: Where to save audio (default: temp file)\n\n        Returns:\n            Path to extracted audio file\n        \"\"\"\n        if output_path is None:\n            output_path = tempfile.mktemp(suffix=\".wav\")\n\n        print(f\"Extracting audio from video...\")\n\n        cmd = [\n            \"ffmpeg\", \"-y\",\n            \"-i\", video_path,\n            \"-vn\",  # No video\n            \"-acodec\", \"pcm_s16le\",  # PCM format for Whisper\n            \"-ar\", \"16000\",  # 16kHz sample rate\n            \"-ac\", \"1\",  # Mono\n            output_path\n        ]\n\n        result = subprocess.run(\n            cmd,\n            capture_output=True,\n            text=True,\n        )\n\n        if result.returncode != 0:\n            raise RuntimeError(f\"ffmpeg failed: {result.stderr}\")\n\n        print(f\"Audio extracted to: {output_path}\")\n        return output_path\n\n    def transcribe_audio(self, audio_path: str) -> list[AudioSegment]:\n        \"\"\"\n        Transcribe audio using Whisper.\n\n        Args:\n            audio_path: Path to audio file\n\n        Returns:\n            List of AudioSegment with timestamps and text\n        \"\"\"\n        print(f\"Transcribing audio with Whisper ({self.whisper_model})...\")\n\n        model = self._get_whisper()\n        segments, info = model.transcribe(\n            audio_path,\n            beam_size=5,\n            word_timestamps=False,\n        )\n\n        print(f\"Detected language: {info.language} (probability: {info.language_probability:.2f})\")\n\n        results = []\n        for segment in segments:\n            results.append(AudioSegment(\n                start_seconds=segment.start,\n                end_seconds=segment.end,\n                text=segment.text.strip(),\n            ))\n\n        print(f\"Transcribed {len(results)} audio segments\")\n        return results\n\n    def _encode_image(self, image: np.ndarray) -> str:\n        \"\"\"Encode image to base64 for Ollama API with high quality.\"\"\"\n        if self.use_png:\n            # PNG: lossless but larger file size\n            _, buffer = cv2.imencode('.png', image)\n        else:\n            # JPEG: configurable quality (default 100 for maximum quality)\n            _, buffer = cv2.imencode('.jpg', image, [cv2.IMWRITE_JPEG_QUALITY, self.image_quality])\n        return base64.b64encode(buffer).decode('utf-8')\n\n    def transcribe_frame(\n        self,\n        image: np.ndarray,\n        prompt: str = \"Transcribe all text visible in this presentation slide. Include headings, bullet points, and any other text. Format it clearly.\",\n    ) -> str:\n        \"\"\"\n        Use local LLM to transcribe/describe a frame.\n        \"\"\"\n        encoded = self._encode_image(image)\n\n        payload = {\n            \"model\": self.vision_model,\n            \"prompt\": prompt,\n            \"images\": [encoded],\n            \"stream\": False,\n        }\n\n        response = requests.post(\n            f\"{self.ollama_url}/api/generate\",\n            json=payload,\n            timeout=self.timeout,\n        )\n        response.raise_for_status()\n\n        return response.json().get(\"response\", \"\")\n\n    def _merge_audio_with_frames(\n        self,\n        frames: list[FrameResult],\n        audio_segments: list[AudioSegment],\n    ) -> list[FrameResult]:\n        \"\"\"\n        Associate audio segments with their corresponding frames based on timestamps.\n        Each frame gets the audio segments that occur between it and the next frame.\n        \"\"\"\n        if not audio_segments:\n            return frames\n\n        for i, frame in enumerate(frames):\n            # Find time range for this frame\n            start_time = frame.timestamp_seconds\n            if i + 1 < len(frames):\n                end_time = frames[i + 1].timestamp_seconds\n            else:\n                # Last frame: include all remaining audio\n                end_time = float('inf')\n\n            # Find audio segments in this time range\n            frame.audio_segments = [\n                seg for seg in audio_segments\n                if seg.start_seconds >= start_time and seg.start_seconds < end_time\n            ]\n\n        return frames\n\n    def process_video(\n        self,\n        video_path: str,\n        sample_interval: int = 30,\n        ignore_regions: Optional[list[tuple[float, float, float, float]]] = None,\n        prompt: Optional[str] = None,\n        output_dir: Optional[str] = None,\n        transcribe_audio: bool = True,\n        transcribe_visuals: bool = True,\n    ) -> TranscriptResult:\n        \"\"\"\n        Process entire video: extract distinct frames, transcribe visuals and audio.\n\n        Args:\n            video_path: Path to video file\n            sample_interval: Check every N frames for changes\n            ignore_regions: Regions to mask for change detection (presenter windows)\n            prompt: Custom prompt for visual transcription\n            output_dir: If provided, save frames as images here\n            transcribe_audio: Whether to transcribe audio with Whisper\n            transcribe_visuals: Whether to transcribe visuals with vision LLM\n\n        Returns:\n            TranscriptResult with frames and audio segments\n        \"\"\"\n        if output_dir:\n            output_path = Path(output_dir)\n            output_path.mkdir(parents=True, exist_ok=True)\n\n        # Extract audio first (if requested)\n        audio_segments = []\n        if transcribe_audio:\n            try:\n                audio_path = self.extract_audio(video_path)\n                audio_segments = self.transcribe_audio(audio_path)\n                # Clean up temp audio file\n                if audio_path.startswith(tempfile.gettempdir()):\n                    os.remove(audio_path)\n            except Exception as e:\n                print(f\"Warning: Audio transcription failed: {e}\")\n                print(\"Continuing with visual transcription only...\")\n\n        # Extract and transcribe frames\n        frames = []\n        for frame_result in self.extract_distinct_frames(\n            video_path, sample_interval, ignore_regions\n        ):\n            if transcribe_visuals:\n                print(f\"  Transcribing frame {frame_result.frame_number} visuals...\")\n                if prompt:\n                    frame_result.transcription = self.transcribe_frame(frame_result.image, prompt)\n                else:\n                    frame_result.transcription = self.transcribe_frame(frame_result.image)\n\n            if output_dir:\n                img_path = output_path / f\"frame_{frame_result.frame_number:06d}.jpg\"\n                cv2.imwrite(str(img_path), frame_result.image)\n\n            frames.append(frame_result)\n\n        # Merge audio with frames\n        frames = self._merge_audio_with_frames(frames, audio_segments)\n\n        return TranscriptResult(frames=frames, audio_segments=audio_segments)\n\n    def save_transcript(\n        self,\n        result: TranscriptResult,\n        output_path: str,\n        include_visuals: bool = True,\n        include_audio: bool = True,\n    ):\n        \"\"\"Save transcription results to a text file.\"\"\"\n        with open(output_path, 'w') as f:\n            f.write(\"VIDEO PRESENTATION TRANSCRIPT\\n\")\n            f.write(\"=\" * 60 + \"\\n\\n\")\n\n            for frame in result.frames:\n                minutes = int(frame.timestamp_seconds // 60)\n                seconds = int(frame.timestamp_seconds % 60)\n\n                f.write(f\"\\n{'='*60}\\n\")\n                f.write(f\"[{minutes:02d}:{seconds:02d}] SLIDE (Frame {frame.frame_number})\\n\")\n                f.write(\"=\" * 60 + \"\\n\")\n\n                if include_visuals and frame.transcription:\n                    f.write(\"\\nðŸ“Š SLIDE CONTENT:\\n\")\n                    f.write(\"-\" * 40 + \"\\n\")\n                    f.write(frame.transcription)\n                    f.write(\"\\n\")\n\n                if include_audio and frame.audio_segments:\n                    f.write(\"\\nðŸŽ¤ SPEAKER AUDIO:\\n\")\n                    f.write(\"-\" * 40 + \"\\n\")\n                    for seg in frame.audio_segments:\n                        seg_min = int(seg.start_seconds // 60)\n                        seg_sec = int(seg.start_seconds % 60)\n                        f.write(f\"[{seg_min:02d}:{seg_sec:02d}] {seg.text}\\n\")\n\n                f.write(\"\\n\")\n\n        print(f\"Transcript saved to: {output_path}\")\n\n    def save_audio_only_transcript(\n        self,\n        result: TranscriptResult,\n        output_path: str,\n    ):\n        \"\"\"Save just the audio transcript (like a subtitle file).\"\"\"\n        with open(output_path, 'w') as f:\n            f.write(\"AUDIO TRANSCRIPT\\n\")\n            f.write(\"=\" * 60 + \"\\n\\n\")\n\n            for seg in result.audio_segments:\n                start_min = int(seg.start_seconds // 60)\n                start_sec = int(seg.start_seconds % 60)\n                end_min = int(seg.end_seconds // 60)\n                end_sec = int(seg.end_seconds % 60)\n\n                f.write(f\"[{start_min:02d}:{start_sec:02d} -> {end_min:02d}:{end_sec:02d}]\\n\")\n                f.write(f\"{seg.text}\\n\\n\")\n\n        print(f\"Audio transcript saved to: {output_path}\")",
   "id": "5b1f1a82c0385c48",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T11:06:49.845362Z",
     "start_time": "2025-12-01T11:06:49.815113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     import sys\n",
    "#\n",
    "#     if len(sys.argv) < 2:\n",
    "#         print(\"Usage: python video_transcriber.py <video_file> [output_dir]\")\n",
    "#         print(\"\\nExample:\")\n",
    "#         print(\"  python video_transcriber.py presentation.mp4 ./output\")\n",
    "#         print(\"\\nTo preview ignore regions:\")\n",
    "#         print(\"  python video_transcriber.py presentation.mp4 --preview\")\n",
    "#         sys.exit(1)\n",
    "#\n",
    "#     video_file = sys.argv[1]\n",
    "#\n",
    "#     # Example ignore regions for presenter windows\n",
    "#     # Adjust these coordinates based on your video layout!\n",
    "#     # Format: (x, y, width, height) as fractions of frame size (0-1)\n",
    "#     ignore_regions = [\n",
    "#         # Example: Bottom-right corner presenter window\n",
    "#         # (0.75, 0.70, 0.25, 0.30),\n",
    "#         # Example: Bottom-left corner presenter window\n",
    "#         # (0.0, 0.70, 0.25, 0.30),\n",
    "#     ]\n",
    "#\n",
    "#     transcriber = VideoTranscriber(\n",
    "#         vision_model=\"llava\",\n",
    "#         whisper_model=\"base\",  # Options: tiny, base, small, medium, large-v3\n",
    "#         similarity_threshold=0.90,\n",
    "#     )\n",
    "#\n",
    "#     # Preview mode\n",
    "#     if \"--preview\" in sys.argv:\n",
    "#         # First, show frame without regions to help identify coordinates\n",
    "#         transcriber.preview_ignore_regions(\n",
    "#             video_file,\n",
    "#             ignore_regions=[],  # Empty to see full frame\n",
    "#             output_path=\"preview_full.jpg\",\n",
    "#         )\n",
    "#         if ignore_regions:\n",
    "#             transcriber.preview_ignore_regions(\n",
    "#                 video_file,\n",
    "#                 ignore_regions,\n",
    "#                 output_path=\"preview_masked.jpg\",\n",
    "#             )\n",
    "#         print(\"\\nCheck the preview images to tune your ignore_regions.\")\n",
    "#         sys.exit(0)\n",
    "#\n",
    "#     output_dir = sys.argv[2] if len(sys.argv) > 2 else \"./video_output\"\n",
    "#\n",
    "#     print(f\"Processing: {video_file}\")\n",
    "#     result = transcriber.process_video(\n",
    "#         video_file,\n",
    "#         sample_interval=30,\n",
    "#         ignore_regions=ignore_regions,\n",
    "#         output_dir=output_dir,\n",
    "#     )\n",
    "#\n",
    "#     # Save combined transcript\n",
    "#     transcriber.save_transcript(result, f\"{output_dir}/transcript.txt\")\n",
    "#\n",
    "#     # Save audio-only transcript\n",
    "#     transcriber.save_audio_only_transcript(result, f\"{output_dir}/audio_transcript.txt\")\n",
    "#\n",
    "#     print(f\"\\nDone! Captured {len(result.frames)} distinct slides.\")\n",
    "#    print(f\"Transcribed {len(result.audio_segments)} audio segments.\")"
   ],
   "id": "27367940a5e2a102",
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3015724656.py, line 65)",
     "output_type": "error",
     "traceback": [
      "  \u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[19]\u001B[39m\u001B[32m, line 65\u001B[39m\n\u001B[31m    \u001B[39m\u001B[31mprint(f\"Transcribed {len(result.audio_segments)} audio segments.\")\u001B[39m\n    ^\n\u001B[31mIndentationError\u001B[39m\u001B[31m:\u001B[39m unexpected indent\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T11:39:53.870146Z",
     "start_time": "2025-12-01T11:07:32.802528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test with cp-demo.mp4 (no audio)\n",
    "video_file = \"../data/cp-demo.mp4\"\n",
    "output_dir = \"../tests/output/cp-demo\"\n",
    "\n",
    "transcriber = VideoTranscriber(\n",
    "    ollama_url=\"http://polwarth:11434\",\n",
    "    vision_model=\"deepseek-ocr:latest\",\n",
    "    whisper_model=\"base\",\n",
    "    similarity_threshold=0.90,\n",
    "    image_quality=100,  # Maximum JPEG quality\n",
    "    use_png=False,  # Set to True for lossless (but larger files)\n",
    "    timeout=600,  # 10 minutes timeout for slow models\n",
    ")\n",
    "\n",
    "# Better prompt for OCR-focused models\n",
    "ocr_prompt = \"\"\"Extract and transcribe ALL text visible in this image. \n",
    "Be precise and accurate. Do not add any interpretation or description.\n",
    "Only output the text you can see, preserving the layout and structure.\"\"\"\n",
    "\n",
    "print(f\"Processing: {video_file}\")\n",
    "result = transcriber.process_video(\n",
    "    video_file,\n",
    "    sample_interval=3,\n",
    "    ignore_regions=[],  # No presenter window to ignore\n",
    "    output_dir=output_dir,\n",
    "    transcribe_audio=False,  # No audio in this video\n",
    "    transcribe_visuals=True,\n",
    "    prompt=ocr_prompt,\n",
    ")\n",
    "\n",
    "# Save transcript\n",
    "transcriber.save_transcript(result, f\"{output_dir}/transcript.txt\")\n",
    "\n",
    "print(f\"\\nDone! Captured {len(result.frames)} distinct slides.\")\n",
    "print(f\"Output saved to: {output_dir}\")"
   ],
   "id": "93174fd01481d93d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ../data/cp-demo.mp4\n",
      "Video: 2186 frames at 15.0 FPS (145.7s / 2.4min)\n",
      "  Captured frame 3 at 0.2s (0.0min)\n",
      "  Transcribing frame 3 visuals...\n",
      "  Captured frame 78 at 5.2s (0.1min)\n",
      "  Transcribing frame 78 visuals...\n",
      "  Captured frame 438 at 29.2s (0.5min)\n",
      "  Transcribing frame 438 visuals...\n",
      "  Captured frame 612 at 40.8s (0.7min)\n",
      "  Transcribing frame 612 visuals...\n",
      "  Captured frame 651 at 43.4s (0.7min)\n",
      "  Transcribing frame 651 visuals...\n",
      "  Captured frame 1359 at 90.6s (1.5min)\n",
      "  Transcribing frame 1359 visuals...\n",
      "  Captured frame 1374 at 91.6s (1.5min)\n",
      "  Transcribing frame 1374 visuals...\n",
      "  Captured frame 1551 at 103.4s (1.7min)\n",
      "  Transcribing frame 1551 visuals...\n",
      "  Captured frame 1707 at 113.8s (1.9min)\n",
      "  Transcribing frame 1707 visuals...\n",
      "Transcript saved to: ../tests/output/cp-demo/transcript.txt\n",
      "\n",
      "Done! Captured 9 distinct slides.\n",
      "Output saved to: ../tests/output/cp-demo\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "0ky5sml0dcfb",
   "source": "# Check video properties (resolution, etc.)\nimport cv2\ncap = cv2.VideoCapture(\"../data/cp-demo.mp4\")\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\ncap.release()\nprint(f\"Video resolution: {width}x{height}\")\nprint(f\"FPS: {fps}\")\nprint(f\"Aspect ratio: {width/height:.2f}\")\nif width < 1280:\n    print(\"âš ï¸ Warning: Low resolution video may affect OCR quality\")",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T11:06:49.867441970Z",
     "start_time": "2025-12-01T10:53:58.229120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video resolution: 1280x972\n",
      "FPS: 15.0\n",
      "Aspect ratio: 1.32\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
